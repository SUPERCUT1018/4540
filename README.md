# 4540

Comparative Analysis of Vision Transformers for Image Classification

Objective: Conducted a comprehensive evaluation of Vision Transformers (ViT), Simple Vision Transformers (SimpleViT), and Performer models on CIFAR-10 and MNIST datasets to analyze trade-offs between accuracy, computational efficiency, and scalability.
Key Contributions:
Performance Evaluation: Assessed models on training time, inference time, and test accuracy, while exploring the impact of various hyperparameters such as depth, embedding dimensions, learning rate, and dropout rate.
Model Insights:
Identified SimpleViT as the most balanced model in terms of accuracy and computational efficiency.
Demonstrated that Performer models, leveraging the FAVOR+ mechanism, achieve linear self-attention complexity with excellent scalability.
Analyzed ViT's potential in capturing global features, despite its higher resource consumption and slightly lower accuracy compared to SimpleViT.
Innovative Findings: Highlighted that Performer models with a learnable kernel function (
ùëì
ùúÉ
f 
Œ∏
‚Äã
 ) maintain strong accuracy while ensuring computational efficiency.
Technical Skills: Worked extensively with deep learning frameworks, hyperparameter tuning, and performance visualization techniques to derive actionable insights.
Outcome: Produced an in-depth analysis of model trade-offs, guiding the selection of architectures based on task-specific requirements and resource constraints.
